\chapter{Introduction}

\section{Historical Context of AI and Neural Networks}

The story of neural networks is inseparable from the broader history of artificial intelligence. In the mid-twentieth century, pioneers began to ask a radical question: could machines learn, reason, and perhaps even think?

Early models of computation and the brain appeared in the 1940s and 1950s. Warren McCulloch and Walter Pitts proposed a mathematical model of the neuron, reducing it to a binary threshold unit. Their work suggested that networks of such units could, in principle, compute any logical function. Around the same time, Alan Turing speculated about ``learning machines,'' planting seeds that would later grow into the foundations of AI.

The perceptron era began in 1957, when Frank Rosenblatt introduced a trainable model capable of learning linear decision boundaries. It captured the imagination of both scientists and the public, sparking optimism that machines could soon replicate the brain’s capacity for learning. Yet mathematics also revealed the perceptron’s limits. In the late 1960s, Marvin Minsky and Seymour Papert proved that single-layer perceptrons could not solve even simple nonlinear problems such as XOR. This was a sobering reminder that without mathematical rigor, bold claims collapse under scrutiny.

These early attempts reveal an important lesson: science moves in cycles of enthusiasm and skepticism. Each generation rediscovers that true progress requires a marriage between creative vision and mathematical clarity.

\section{AI Winters and the Deep Learning Revolution}

The collapse of optimism after the perceptron marked the first ``AI winter'' of the 1970s. Funding dried up, and public interest waned. Limited computing power, scarce data, and inflated promises led many to dismiss neural networks as a dead end. A second AI winter followed in the late 1980s, as symbolic methods, once thought to be the future of AI, also struggled to deliver.

Yet beneath the surface, mathematics was preparing a renaissance. In the 1980s, the backpropagation algorithm was formalized and popularized, allowing multilayer perceptrons to model complex nonlinear functions. Still, adoption was slow, because hardware had not yet caught up with theory. Neural networks were powerful on paper, but impractical in real-world applications.

The deep learning explosion of the 2010s changed everything. With the rise of GPUs, massive datasets, and architectures such as convolutional and recurrent networks, machines suddenly outperformed classical methods in vision, language, and speech. Soon after, transformers redefined the field altogether, enabling large-scale models that blurred the line between statistics and creativity. At the core of these breakthroughs was not magic, but mathematics: linear algebra for representation, probability for modeling uncertainty, and optimization theory for training vast networks.

The history of neural networks is therefore not merely technical—it is also the history of human patience. What once looked like failure was in fact a pause, waiting for mathematics and technology to converge.

\section{Why Mathematics Matters in Deep Learning}

If philosophy gave us the first questions about intelligence, mathematics gave us the tools to answer them. Galileo once wrote that the universe ``is written in the language of mathematics, and its characters are triangles, circles, and other geometrical figures.'' In the same spirit, deep learning is written in the language of vectors, matrices, and functions. Every model is a translation from the world’s complexity into mathematical form, and every training process is an attempt to solve an equation that nature has posed.

Linear algebra is the grammar of representation, calculus is the machinery of change, probability is the measure of uncertainty, and optimization is the path to improvement. Without them, neural networks would be shapeless intuitions. With them, they become structured systems capable of learning patterns from the world.

Mathematics is therefore not a peripheral tool but the very skeleton of deep learning. It gives rigor to vision, coherence to creativity, and structure to intuition. Just as the Greeks once sought logic to discipline thought, we now rely on mathematics to discipline learning.

This book begins here—at the intersection of history, philosophy, and mathematics—because to understand neural networks is not merely to know how they work, but to see them as part of humanity’s timeless attempt to comprehend intelligence itself.


