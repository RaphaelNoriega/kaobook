\chapter{Optimization Theory}
\section{Convexity, duality, and Lagrangians}
% TODO
\subsection{Convex sets and convex functions}
\subsection{First- and second-order conditions for convexity}
\subsection{Lagrange multipliers}
\subsection{Karush–Kuhn–Tucker (KKT) conditions}
\subsection{Strong and weak duality}

\section{Gradient descent and its variants}
% TODO
\subsection{Steepest descent method}
\subsection{Learning rate selection and scheduling}
\subsection{Momentum-based methods}
\subsection{Nesterov accelerated gradient (NAG)}
\subsection{Adaptive methods (AdaGrad, RMSProp, Adam)}

\section{Stochastic optimization and convergence}
% TODO
\subsection{Stochastic gradient descent (SGD)}
\subsection{Mini-batch training}
\subsection{Convergence criteria and analysis}
\subsection{Variance reduction techniques}
\subsection{Generalization vs optimization trade-offs}

\section{Newton and quasi-Newton methods}
% TODO
\subsection{Newton's method for optimization}
\subsection{Hessian-based updates}
\subsection{BFGS and L-BFGS algorithms}
\subsection{Trust-region methods}
\subsection{Comparisons with first-order methods}

\section{Variational Principles in Optimization and Learning}
\subsection{Variational formulations of optimization problems}
\subsection{Euler–Lagrange equations in optimization}
\subsection{Energy minimization perspectives}
\subsection{Connections to physics-informed learning (PINNs)}
