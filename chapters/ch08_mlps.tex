\chapter{Feedforward Networks (MLPs)}
\section{Activation functions (ReLU, sigmoid, tanh, GELU, softmax)}
% TODO

\section{Universal Approximation Theorem}
% TODO

\section{Forward and backward propagation}
% TODO

\section{Vanishing and exploding gradients}
% TODO


