\chapter{Feedforward Networks (MLPs)}
\section{Activation Functions (ReLU, sigmoid, tanh, GELU, softmax)}
\section{Universal Approximation Theorem}
\section{Forward and Backward Propagation}
\section{Initialization, Normalization \& Gradient Dynamics}
  \subsection{Weight Initialization (Xavier/He, LSUV, Orthogonal)}
  \subsection{Normalization Layers (Batch, Layer, Group)}
  \subsection{Gradient Flow Diagnostics (Activation/Weight Stats)}
  \subsection{Learning Rate Schedules \& Warmup}
\section{Vanishing and Exploding Gradients}


