\chapter{Recurrent Neural Networks (RNNs)}

\section{Sequences as Dynamical Systems}
\subsection{Sequence modeling motivation}
\subsection{Recurrent computation and state updates}
\subsection{Unrolling through time}
\subsection{Training with backpropagation through time (BPTT)}
\subsection{Limitations of basic RNNs}

\section{Gradient Vanishing and Exploding}
\subsection{Analysis of gradient propagation over time steps}
\subsection{Vanishing gradients and long-term dependencies}
\subsection{Exploding gradients and instability}
\subsection{Gradient clipping techniques}
\subsection{Initialization strategies to mitigate gradient issues}

\section{LSTMs and GRUs}
\subsection{Long Short-Term Memory (LSTM) architecture}
\subsection{Input, forget, and output gates}
\subsection{Gated Recurrent Unit (GRU) architecture}
\subsection{Comparison of LSTMs and GRUs}
\subsection{Handling long-term dependencies}

\section{Applications in NLP, Speech, Time-Series}
\subsection{Language modeling and text generation}
\subsection{Machine translation}
\subsection{Speech recognition and synthesis}
\subsection{Time-series forecasting}
\subsection{Sensor data and sequence classification}


