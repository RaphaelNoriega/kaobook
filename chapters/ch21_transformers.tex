\chapter{Transformers and Attention Mechanisms}
\section{Self-attention: queries, keys, values}
% TODO

\section{Multi-head attention}
% TODO

\section{Positional encodings}
% TODO

\section{Transformer architectures: BERT, GPT, multimodal}
% TODO

\section{Applications in PDEs and symbolic regression}
% TODO


