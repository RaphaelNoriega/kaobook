\chapter{Transformers and Attention Mechanisms}
\section{Self-Attention: Queries, Keys, Values}
\subsection{Motivation for attention over recurrence}
\subsection{Query-key-value formulation}
\subsection{Scaled dot-product attention}
\subsection{Attention weights and softmax normalization}
\subsection{Computational complexity considerations}

\section{Multi-Head Attention}
\subsection{Parallel attention heads}
\subsection{Linear projections and concatenation}
\subsection{Benefits of multi-head structure}
\subsection{Implementation details}
\subsection{Visualization and interpretability}

\section{Positional Encodings}
\subsection{Need for positional information in sequences}
\subsection{Sinusoidal positional encodings}
\subsection{Learned positional embeddings}
\subsection{Incorporation into transformer architecture}
\subsection{Impact on long-range dependencies}

\section{Transformer Architectures: BERT, GPT, Multimodal}
\subsection{Encoder-decoder structure of the original Transformer}
\subsection{BERT: bidirectional encoder representations}
\subsection{GPT: autoregressive decoder-only models}
\subsection{Vision transformers (ViTs) and multimodal transformers}
\subsection{Scaling laws and large language models}

\section{Applications in PDEs and Symbolic Regression}
\subsection{Transformers for solving partial differential equations}
\subsection{Sequence modeling of discretized fields}
\subsection{Neural operators and Fourier transformers}
\subsection{Symbolic regression with attention models}
\subsection{Scientific discovery and equation learning}

