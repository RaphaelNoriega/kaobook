\chapter{Deep Reinforcement Learning}
\section{Deep Q-Networks (DQN)}
\subsection{Limitations of tabular Q-learning}
\subsection{Neural network function approximation for Q-values}
\subsection{Experience replay and target networks}
\subsection{Training stability techniques}
\subsection{Extensions: Double DQN, Dueling DQN, Prioritized Replay}

\section{Policy Gradient Methods (REINFORCE, PPO)}
\subsection{Policy-based vs value-based approaches}
\subsection{REINFORCE algorithm and derivation}
\subsection{High variance and variance reduction techniques}
\subsection{Proximal Policy Optimization (PPO) algorithm}
\subsection{Trust region methods and clipping objectives}

\section{Actor--Critic Architectures (A2C, A3C)}
\subsection{Concept of actor and critic networks}
\subsection{Advantage functions and baseline subtraction}
\subsection{Asynchronous advantage actor-critic (A3C)}
\subsection{Synchronous advantage actor-critic (A2C)}
\subsection{Sample efficiency and stability considerations}

\section{Landmark Systems: AlphaGo, AlphaZero, MuZero}
\subsection{AlphaGo: combining deep neural networks and MCTS}
\subsection{AlphaZero: self-play reinforcement learning}
\subsection{MuZero: learning dynamics models from scratch}
\subsection{Architectural innovations and scalability}
\subsection{Impact on AI research and real-world applications}
