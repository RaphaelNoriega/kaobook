\chapter{Classical Reinforcement Learning}
\section{Agents, Environments, States, Actions, Rewards}
\subsection{Reinforcement learning problem setup}
\subsection{Agent-environment interaction loop}
\subsection{Definition of states, actions, and rewards}
\subsection{Exploration vs exploitation}
\subsection{Episodic vs continuing tasks}

\section{Markov Decision Processes (MDPs)}
\subsection{Markov property and state transitions}
\subsection{Transition probability matrices}
\subsection{Reward functions and discount factors}
\subsection{Policy definition and representation}
\subsection{Formulating RL as an MDP}

\section{Value Functions and Bellman Equations}
\subsection{State-value and action-value functions}
\subsection{Bellman expectation equations}
\subsection{Bellman optimality equations}
\subsection{Relationship between value functions and policies}
\subsection{Policy evaluation and improvement}

\section{Tabular Methods: SARSA, Q-Learning}
\subsection{On-policy learning with SARSA}
\subsection{Off-policy learning with Q-learning}
\subsection{Temporal difference (TD) learning}
\subsection{Exploration strategies (epsilon-greedy, softmax)}
\subsection{Convergence properties of tabular methods}


